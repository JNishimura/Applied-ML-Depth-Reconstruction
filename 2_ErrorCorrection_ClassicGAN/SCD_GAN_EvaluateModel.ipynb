{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.6/dist-packages (3.2.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.15.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.10.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (4.50.2)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.18.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (2.24.0)\n",
      "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.24.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.17.3)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.12.1)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (20.2.0)\n",
      "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /iahome/b/br/broschwartz/.local/lib/python3.6/site-packages (from tensorflow_datasets) (3.15.6)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.52.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in /iahome/b/br/broschwartz/.local/lib/python3.6/site-packages (3.3.4)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.17.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /iahome/b/br/broschwartz/.local/lib/python3.6/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /iahome/b/br/broschwartz/.local/lib/python3.6/site-packages (from matplotlib) (8.1.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /iahome/b/br/broschwartz/.local/lib/python3.6/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pillow in /iahome/b/br/broschwartz/.local/lib/python3.6/site-packages (8.1.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q git+https://github.com/tensorflow/examples.git\n",
    "!pip install tensorflow_datasets\n",
    "!pip install matplotlib\n",
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "predicted_path = '../Data/Dataset/Predicted_Depth/'\n",
    "original_path = '../Data/Dataset/Original_Depth/'\n",
    "models_path = '../Models/GANs/'\n",
    "model_name = 'Apr-27-2021_error_generator'\n",
    "testing_append = 'Testing/'\n",
    "training_append = 'Training/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Found 230 files belonging to 1 classes.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "<BatchDataset shapes: (32, 512, 512, 1), types: tf.float32>\n",
      "<class 'str'>\n",
      "Found 230 files belonging to 1 classes.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    }
   ],
   "source": [
    "# DATASET PARAMETERS\n",
    "SEED = 123\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 1000\n",
    "IMG_SIZE = 512\n",
    "# DATASET CONFIGS\n",
    "ground_truth_path = original_path + testing_append\n",
    "# training_set_path = dataset_path\n",
    "print(type(ground_truth_path))\n",
    "# !nvidia-smi --query-gpu=memory.used --format=csv\n",
    "ground_truth_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    ground_truth_path,label_mode=None,\n",
    "    class_names=None, color_mode='grayscale', batch_size=BATCH_SIZE, image_size=(IMG_SIZE,\n",
    "    IMG_SIZE), shuffle=False, seed=SEED, validation_split=None, subset=None,\n",
    "    interpolation='bilinear', follow_links=False\n",
    ")\n",
    "# !nvidia-smi --query-gpu=memory.used --format=csv\n",
    "ground_truth_dataset = ground_truth_dataset.unbatch()\n",
    "# ground_truth_dataset = ground_truth_dataset.shuffle(BUFFER_SIZE, seed=SEED)\n",
    "ground_truth_dataset = ground_truth_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(ground_truth_dataset)\n",
    "\n",
    "testing_data_path = predicted_path + testing_append\n",
    "print(type(testing_data_path))\n",
    "testing_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    testing_data_path,label_mode=None,\n",
    "    class_names=None, color_mode='grayscale', batch_size=BATCH_SIZE, image_size=(IMG_SIZE,\n",
    "    IMG_SIZE), shuffle=False, seed=SEED, validation_split=None, subset=None,\n",
    "    interpolation='bilinear', follow_links=False\n",
    ")\n",
    "testing_dataset = testing_dataset.unbatch()\n",
    "# testing_dataset = testing_dataset.shuffle(BUFFER_SIZE, seed=SEED)\n",
    "testing_dataset = testing_dataset.batch(BATCH_SIZE, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "    \n",
    "def normalize(input_image: tf.Tensor) -> tuple:\n",
    "    \"\"\"Rescale the pixel values of the images between 0.0 and 1.0\n",
    "    compared to [0,255] originally.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_image : tf.Tensor\n",
    "        Tensorflow tensor containing an image of size [SIZE,SIZE,3].\n",
    "    input_mask : tf.Tensor\n",
    "        Tensorflow tensor containing an annotation of size [SIZE,SIZE,1].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Normalized image and its annotation.\n",
    "    \"\"\"\n",
    "    input_image = (tf.cast(input_image, tf.float32))\n",
    "    input_image = (input_image - tf.math.reduce_min(input_image))\n",
    "    input_image = input_image / tf.math.reduce_max(input_image)\n",
    "    return input_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xyzmwrite(path, x, y, z, r, g, b, m):\n",
    "    w, h = r.shape # This seems to be a problem with the naming scheme in the original MATLAB xyzmwrite, or I may just be dumb\n",
    "\n",
    "    xyz = np.zeros((w*h, 3))\n",
    "\n",
    "    xm1 = np.array(np.reshape(x, w*h, order = 'F'))\n",
    "    ym1 = np.array(np.reshape(y, w*h, order = 'F'))\n",
    "    zm1 =  np.array(np.reshape(z, w*h, order = 'F'))\n",
    "    xyz[:, 0] = xm1\n",
    "    xyz[:, 1] = ym1\n",
    "    xyz[:, 2] = zm1\n",
    "    xyz = np.reshape(xyz.T, w*h*3, order = 'F')\n",
    "\n",
    "    rgb = np.zeros((w*h, 3))\n",
    "    r1 = np.array(np.reshape(r, w*h, order = 'F'))\n",
    "    g1 = np.array(np.reshape(g, w*h, order = 'F'))\n",
    "    b1 =  np.array(np.reshape(b, w*h, order = 'F'))\n",
    "    rgb[:, 0] = r1\n",
    "    rgb[:, 1] = g1\n",
    "    rgb[:, 2] = b1\n",
    "    rgb = np.reshape(rgb.T, w*h*3, order = 'F')\n",
    "\n",
    "    m = np.reshape(m, w*h, order = 'F')\n",
    "\n",
    "    try:\n",
    "        os.remove(path)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    with open(path, 'wb') as xyzm_file:\n",
    "        xyzm_file.write(b'image size width x height = %d x %d'%(w, h))\n",
    "        xyzm_file.write(xyz.astype(np.float32))\n",
    "        xyzm_file.write(rgb.astype(np.uint8))\n",
    "        xyzm_file.write(m.astype(np.uint8))\n",
    "\n",
    "    return None\n",
    "\n",
    "def xyzmread(path):\n",
    "    # Find height and width of data from header\n",
    "    file_object = open(path, 'rb')                                              # Open file\n",
    "    file_header = file_object.read(100).decode('ascii', errors = 'ignore')      # Get header as ASCII string\n",
    "    file_header = ''.join(filter(lambda x: x in string.printable, file_header)) # Filter non-printable characters out\n",
    "    file_header_info = file_header.split('=')                                        \n",
    "    file_header_info = file_header_info[1].split('x')\n",
    "    width = int(file_header_info[0])\n",
    "    height_string = file_header_info[1]\n",
    "    valid_height_string = ''\n",
    "    for i in range(1,len(height_string)):                                       # Range 1-len(height_string) ignores the space after the 'x'\n",
    "        if (ord(height_string[i]) < 48 or ord(height_string[i]) > 57):          # Do not add to valid string if values lie outside '0' - '9'\n",
    "            break\n",
    "        valid_height_string = valid_height_string + height_string[i]\n",
    "    height = int(valid_height_string)\n",
    "\n",
    "    # Ignore header in data\n",
    "    # Additional 1's added to reflect the removal of characters in .split() method\n",
    "    header_length = len(file_header.split('x')[0]) + 1 + len(file_header.split('x')[1]) + 1 + len(valid_height_string) + 1\n",
    "    file_object.seek(header_length, 0)\n",
    "    # Read in data\n",
    "    xyz_data = []\n",
    "    rgb_data = []\n",
    "    m_data = []\n",
    "    for i in range(0, (width*height*3)):\n",
    "        (num,) = struct.unpack('f', file_object.read(4))\n",
    "        xyz_data.append(num)\n",
    "\n",
    "    for j in range(0, (width*height*3)):\n",
    "        (num,) = struct.unpack('>H', b'\\x00' + file_object.read(1))\n",
    "        rgb_data.append(num)\n",
    "    \n",
    "    for k in range(0, (width*height)):\n",
    "        (num,) = struct.unpack('>H', b'\\x00' + file_object.read(1))\n",
    "        m_data.append(num)\n",
    "\n",
    "    xyz_data = np.reshape(xyz_data, (3, width*height), order = 'F')\n",
    "    x_data = xyz_data[0,:]\n",
    "    x_data = np.reshape(x_data, (width, height), order = 'F')\n",
    "    y_data = xyz_data[1,:]\n",
    "    y_data = np.reshape(y_data, (width, height), order = 'F')\n",
    "    z_data = xyz_data[2,:]\n",
    "    z_data = np.reshape(z_data, (width, height), order = 'F')\n",
    "\n",
    "    rgb_data = np.reshape(rgb_data, (3, width*height), order = 'F')\n",
    "    r_data = rgb_data[0,:]\n",
    "    r_data = np.reshape(r_data, (width, height), order = 'F')\n",
    "    g_data = rgb_data[1,:]\n",
    "    g_data = np.reshape(g_data, (width, height), order = 'F')\n",
    "    b_data = rgb_data[2,:]\n",
    "    b_data = np.reshape(b_data, (width, height), order = 'F')\n",
    "\n",
    "    m_data = np.reshape(m_data, (width, height), order = 'F')\n",
    "\n",
    "    return x_data, y_data, z_data, r_data, g_data, b_data, m_data\n",
    "\n",
    "def xyzmshow2d(x, y, z, r, g, b, m):\n",
    "    print('Preparing 2D...')\n",
    "    # plt.ion()\n",
    "    # plt.show()\n",
    "\n",
    "    fig, axs = plt.subplots(3,3)\n",
    "    axs[0, 0].imshow(x)\n",
    "    axs[0, 0].set_title('X')\n",
    "    axs[0, 1].imshow(y)\n",
    "    axs[0, 1].set_title('Y')\n",
    "    axs[0, 2].imshow(z)\n",
    "    axs[0, 2].set_title('Z')\n",
    "    axs[1, 0].imshow(r)\n",
    "    axs[1, 0].set_title('R')\n",
    "    axs[1, 1].imshow(g)\n",
    "    axs[1, 1].set_title('G')\n",
    "    axs[1, 2].imshow(b)\n",
    "    axs[1, 2].set_title('B')\n",
    "    axs[2, 0].imshow(m)\n",
    "    axs[2, 0].set_title('M')\n",
    "    fig.delaxes(axs[2,1])\n",
    "    fig.delaxes(axs[2,2]) \n",
    "\n",
    "    # plt.draw()\n",
    "    # plt.pause(0.001)\n",
    "    print('Displaying 2D...')\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(display_list):\n",
    "    plt.figure()\n",
    "\n",
    "    title = ['Input Image', 'Corrected Depth']\n",
    "\n",
    "#     for i in range(len(display_list)):\n",
    "# #         plt.subplot(1, len(display_list), i+1)\n",
    "#         plt.title(title[i])\n",
    "# #         data = data.reshape((display_list[i].shape[0], display_list[i].shape[1], 1)\n",
    "#         plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[0]))\n",
    "#         d = display_list[1]\n",
    "#         plt.imshow(d[1, :, :, 0]/255.0, cmap = 'gray')\n",
    "#         # plt.axis('off')\n",
    "#     print(np.max(display_list[0]))\n",
    "#     print(np.min(display_list[0]))\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[0]))\n",
    "    d = display_list[1]\n",
    "#     print(d.shape)\n",
    "#     print(np.max(d))\n",
    "#     print(np.min(d))\n",
    "    plt.show()\n",
    "    plt.figure()\n",
    "    plt.imshow(d * 127.5 + 127.5, cmap = 'gray')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def show_predictions(dataset=None, num=1):\n",
    "    if dataset:\n",
    "        for image, depth in dataset.take(num):\n",
    "            pred_mask = model.predict(image)\n",
    "            display([image[0], depth[0], create_mask(pred_mask)])\n",
    "    else:\n",
    "        display([sample_image, sample_mask, create_mask(model.predict(sample_image[tf.newaxis, ...]))])\n",
    "        \n",
    "def evaluate_predictions(dataset=None, num=1):\n",
    "    if dataset:\n",
    "        for image in dataset.take(num):\n",
    "            corr_depth = model.predict(image)\n",
    "#             mask = create_mask(pred_mask)\n",
    "#             decoded_depth = scdDecode(image[0], mask, depth[0], verbose = True)\n",
    "            \n",
    "#             print('Actual', 'Min Depth:', np.min(depth[0]), 'Max Depth:', np.max(depth[0]))\n",
    "#             print('Predicted', 'Min Depth:', np.min(decoded_depth), 'Max Depth:', np.max(decoded_depth))\n",
    "            display([image[0,:,:,:], corr_depth[0,:,:,:]])\n",
    "            \n",
    "def evaluate_all_data(dataset=None, save_images=False, save_testing=False):\n",
    "      \n",
    "    if dataset:\n",
    "        num_images = 0\n",
    "        total_sum_squared_diff = 0\n",
    "        for batch in dataset:\n",
    "            images = batch\n",
    "#             depths = batch[1]\n",
    "            \n",
    "            for i in range(0, len(images)):\n",
    "                image = images[i,:,:,:]\n",
    "#                 depth = depths[i]\n",
    "#                 print(image.shape)\n",
    "                image = tf.reshape(image, [1, IMG_SIZE, IMG_SIZE, 1]) # -1 to get None first dimensions         \n",
    "                \n",
    "                corrected_depth = model.predict(image)\n",
    "#                 mask = create_mask(corrected_depth)\n",
    "                corrected_depth = tf.squeeze(corrected_depth,0)\n",
    "                image = tf.squeeze(image,0)\n",
    "                if save_images:\n",
    "                    # Save Predicted Depth\n",
    "                    corrected_depth_image = tf.keras.preprocessing.image.array_to_img(corrected_depth)\n",
    "                    plt.imsave('../Data/Dataset/Analysis_Outputs/Corrected_Depth_Classic_GAN/Testing/' + str(num_images + i + 1) + '_SCD_Corrected_Depth.png', corrected_depth_image)\n",
    "                    # Save Original Depth input\n",
    "                    original_depth_image = tf.keras.preprocessing.image.array_to_img(image)\n",
    "                    plt.imsave('../Data/Dataset/Analysis_Outputs/Original_Depth/Testing/' + str(num_images + i + 1) + '_SCD_Original_Depth.png', original_depth_image)\n",
    "                    \n",
    "                    x = np.linspace(0, 512, 512)\n",
    "                    y = x\n",
    "                    xx, yy = np.meshgrid(x, y)\n",
    "                    xyzmwrite('../Data/Dataset/Analysis_Outputs/Corrected_Depth_Classic_GAN/Testing/' + str(num_images + i + 1) + '_SCD_Corrected_Depth.xyzm', xx, yy, corrected_depth_image, np.zeros([512, 512]), np.zeros([512, 512]), np.zeros([512, 512]), np.ones([512, 512]))\n",
    "\n",
    "                # Calc RMS error for whole dataset\n",
    "                corrected_depth = corrected_depth*127.5 + 127.5\n",
    "                image = image*127.5 + 127.5\n",
    "                diff = corrected_depth - image\n",
    "#                 print(np.max(diff) - np.min(diff))\n",
    "                if save_images:\n",
    "                    diff_image = tf.keras.preprocessing.image.array_to_img(diff)\n",
    "                    plt.imsave('../Data/Dataset/Analysis_Outputs/Diff_Image_Classic_GAN/Testing/' + str(num_images + i + 1) + '_SCD_Diff_Image.png', diff_image)\n",
    "                squared_diff = np.power(diff, 2)\n",
    "                sum_squared_diff = np.sum(squared_diff)\n",
    "                total_sum_squared_diff = total_sum_squared_diff + sum_squared_diff\n",
    "                \n",
    "            num_images = num_images + i + 1\n",
    "            \n",
    "        num_points = IMG_SIZE * IMG_SIZE * num_images\n",
    "        rms_error = np.sqrt(total_sum_squared_diff / num_points)     \n",
    "        print('Number of Images Tested: ', num_images)\n",
    "        print('RMS Error: ', rms_error)\n",
    "\n",
    "    \n",
    "                \n",
    "def calc_rms_error(true_depth, pred_depth):\n",
    "    diff = pred_depth - true_depth\n",
    "    squared_diff = np.power(diff, 2)\n",
    "    sum_squared_diff = np.sum(squared_diff)\n",
    "    \n",
    "    num_points = pred_depth.size\n",
    "    \n",
    "    return np.sqrt(sum_squared_diff / num_points)\n",
    "\n",
    "def calc_avg_abs_error(true_depth, pred_depth):\n",
    "    abs_error = np.abs(pred_depth - true_depth)\n",
    "    avg_abs_error = np.sum(abs_error) / pred_depth.size\n",
    "    \n",
    "    return avg_abs_error\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(models_path + model_name, compile=False)\n",
    "ground_truth_dataset = ground_truth_dataset.map(normalize) # Normalize the images to [0, 1]\n",
    "testing_dataset = testing_dataset.map(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_predictions(testing_dataset, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Images Tested:  224\n",
      "RMS Error:  12.174309938020574\n"
     ]
    }
   ],
   "source": [
    "evaluate_all_data(ground_truth_dataset, save_images=True, save_testing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
