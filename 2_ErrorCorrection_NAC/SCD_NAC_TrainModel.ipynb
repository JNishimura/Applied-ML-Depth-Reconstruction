{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a bunch of happy little packages\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_probability as tfp\n",
    "import time\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sizes / variables used throughout the code\n",
    "\n",
    "# Image size that we are going to use\n",
    "IMG_SIZE = 512\n",
    "# Our images only utilize two channels\n",
    "N_CHANNELS = 3\n",
    "# Beta map has 4 values + background\n",
    "N_CLASSES = 5\n",
    "\n",
    "BATCH_SIZE = 34 # MUST DIVIDE INTO DATASET EVENLY\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "# Seed for random number gen\n",
    "SEED = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/s-l011/local/vol02/k/kubicek/Documents/AML_project/tcd-dcnn/Experiments/TX_Faces/2_ErrorCorrection_NAC\n"
     ]
    }
   ],
   "source": [
    "# Ensure correct directory\n",
    "current_directory = os.getcwd()\n",
    "print(current_directory)\n",
    "dataset_path_clean = current_directory +'/../Data/Dataset/Original_Depth/'\n",
    "dataset_path_error = current_directory + '/../Data/Dataset/Predicted_Depth/'\n",
    "training_data = 'Training/'\n",
    "testing_data = 'Testing/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data / Depth Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Found 919 files belonging to 1 classes.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "<BatchDataset shapes: (34, 512, 512, 1), types: tf.float32>\n",
      "<class 'str'>\n",
      "Found 230 files belonging to 1 classes.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    }
   ],
   "source": [
    "# Load clean training data\n",
    "clean_training_set_path = dataset_path_clean + training_data\n",
    "# training_set_path = dataset_path\n",
    "print(type(clean_training_set_path))\n",
    "\n",
    "clean_train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    clean_training_set_path,label_mode=None,\n",
    "    class_names=None, color_mode='grayscale', batch_size=BATCH_SIZE, image_size=(IMG_SIZE,\n",
    "    IMG_SIZE), shuffle=True, seed=SEED, validation_split=None, subset=None,\n",
    "    interpolation='bilinear', follow_links=False\n",
    ")\n",
    "\n",
    "clean_train_dataset = clean_train_dataset.unbatch()\n",
    "clean_train_dataset = clean_train_dataset.shuffle(BUFFER_SIZE, seed=SEED)\n",
    "clean_train_dataset = clean_train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(clean_train_dataset)\n",
    "\n",
    "# Load clean testing data\n",
    "clean_testing_set_path = dataset_path_clean + testing_data\n",
    "print(type(clean_testing_set_path))\n",
    "\n",
    "clean_test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    clean_testing_set_path,label_mode=None,\n",
    "    class_names=None, color_mode='grayscale', batch_size=BATCH_SIZE, image_size=(IMG_SIZE,\n",
    "    IMG_SIZE), shuffle=True, seed=SEED, validation_split=None, subset=None,\n",
    "    interpolation='bilinear', follow_links=False\n",
    ")\n",
    "clean_test_dataset = clean_test_dataset.unbatch()\n",
    "clean_test_dataset = clean_test_dataset.shuffle(BUFFER_SIZE, seed=SEED)\n",
    "clean_test_dataset = clean_test_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Data / Depth Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Found 919 files belonging to 1 classes.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "<BatchDataset shapes: (34, 512, 512, 1), types: tf.float32>\n",
      "<class 'str'>\n",
      "Found 230 files belonging to 1 classes.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n",
      "WARNING:tensorflow:The operation `tf.image.convert_image_dtype` will be skipped since the input and output dtypes are identical.\n"
     ]
    }
   ],
   "source": [
    "# Load clean training data\n",
    "predicted_training_set_path = dataset_path_error + training_data\n",
    "# training_set_path = dataset_path\n",
    "print(type(predicted_training_set_path))\n",
    "\n",
    "error_train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    predicted_training_set_path,label_mode=None,\n",
    "    class_names=None, color_mode='grayscale', batch_size=BATCH_SIZE, image_size=(IMG_SIZE,\n",
    "    IMG_SIZE), shuffle=True, seed=SEED, validation_split=None, subset=None,\n",
    "    interpolation='bilinear', follow_links=False\n",
    ")\n",
    "\n",
    "error_train_dataset = error_train_dataset.unbatch()\n",
    "error_train_dataset = error_train_dataset.shuffle(BUFFER_SIZE, seed=SEED)\n",
    "error_train_dataset = error_train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(error_train_dataset)\n",
    "\n",
    "# Load clean testing data\n",
    "predicted_testing_set_path = dataset_path_error + testing_data\n",
    "print(type(predicted_testing_set_path))\n",
    "\n",
    "error_test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    predicted_testing_set_path,label_mode=None,\n",
    "    class_names=None, color_mode='grayscale', batch_size=BATCH_SIZE, image_size=(IMG_SIZE,\n",
    "    IMG_SIZE), shuffle=True, seed=SEED, validation_split=None, subset=None,\n",
    "    interpolation='bilinear', follow_links=False\n",
    ")\n",
    "error_test_dataset = error_test_dataset.unbatch()\n",
    "error_test_dataset = error_test_dataset.shuffle(BUFFER_SIZE, seed=SEED)\n",
    "error_test_dataset = error_test_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def normalize(input_image: tf.Tensor) -> tuple:\n",
    "    \"\"\"Rescale the pixel values of the images between 0.0 and 1.0\n",
    "    compared to [0,255] originally.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_image : tf.Tensor\n",
    "        Tensorflow tensor containing an image of size [SIZE,SIZE,3].\n",
    "    input_mask : tf.Tensor\n",
    "        Tensorflow tensor containing an annotation of size [SIZE,SIZE,1].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Normalized image and its annotation.\n",
    "    \"\"\"\n",
    "    input_image = (tf.cast(input_image, tf.float32)) / 255.0\n",
    "    return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images supposedly read in\n"
     ]
    }
   ],
   "source": [
    "# Normalize all data \n",
    "clean_train_dataset = clean_train_dataset.map(normalize) # Normalize the images to [-1, 1]\n",
    "clean_test_dataset = clean_test_dataset.map(normalize)\n",
    "error_train_dataset = error_train_dataset.map(normalize)\n",
    "error_test_dataset = error_test_dataset.map(normalize)\n",
    "print('Images supposedly read in')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some little helper function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Keras Functional API -- #\n",
    "# -- UNet Implementation -- #\n",
    "# Everything here is from tensorflow.keras.layers\n",
    "# I imported tensorflow.keras.layers * to make it easier to read\n",
    "dropout_rate = 0.5\n",
    "input_size = (IMG_SIZE, IMG_SIZE, N_CHANNELS)\n",
    "def show_batch(image_batch):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for n in range(25):\n",
    "        ax = plt.subplot(5,5,n+1)\n",
    "        image = image_batch[n]\n",
    "#         channel = image[:,:,0]\n",
    "        plt.imshow(image)\n",
    "        print(image.shape)\n",
    "        plt.axis('off')\n",
    "        return plt.show()\n",
    "\n",
    "# image_batch = next(iter(train_dataset))\n",
    "# show_batch(image_batch)\n",
    "\n",
    "def get_image(image_batch):\n",
    "    for n in range(25):\n",
    "        image = image_batch[n]\n",
    "        return image\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maybe this is the model definition\n",
    "\n",
    "Taken from https://github.com/wbhu/DnCNN-tensorflow/blob/master/model.py , if this actually works that'd be surprising cuz I have no idea what I am doing. \n",
    "\n",
    "Well, also taken from https://github.com/csjunxu/Noisy-As-Clean-TIP2020/blob/master/models/dncnn.py because the above was stupid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make DnCNN network. This is a 17 layer CNN with ReLu activation functions\n",
    "def make_DnCNN_model():\n",
    "    kernel_size = 3\n",
    "    padding = 1\n",
    "    features = 64/2\n",
    "    c = 1\n",
    "    num_layers = 4\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # Layer 1\n",
    "    model.add(layers.Conv2D(filters=features, kernel_size=(3,3), use_bias=False, padding='same', activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, c)))\n",
    "    \n",
    "    # This could prolly be a 'for' loop but oh well\n",
    "    for _ in range(num_layers-2):\n",
    "        model.add(layers.Conv2D(filters=features, kernel_size=(3,3), use_bias=False, padding='same', activation=None))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.ReLU())\n",
    "\n",
    "    # Layer 17\n",
    "    model.add(layers.Conv2D(filters=c, kernel_size=3, padding='same', use_bias=False))\n",
    "    \n",
    "    # Show model summary\n",
    "    #model.summary()  # uncomment for model summary\n",
    "    print('Yayyy! We have a model :)')\n",
    "    return model\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This might be a training loop - maybe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yayyy! We have a model :)\n"
     ]
    }
   ],
   "source": [
    "DnCNN = make_DnCNN_model()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "# loss=tf.keras.losses.MeanSquaredError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint saver?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'NAC_outputs/training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(DnCNN=DnCNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "#   print(\"TEST INPUT SHAPE\")\n",
    "#   print(test_input.shape)\n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(3,3))\n",
    "#   print(\"PREDICTIONS SHAPE\")\n",
    "#   print(predictions.shape)\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(3, 3, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.savefig('NAC_outputs/epoch_images/image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Salt and pepper noise defintion\n",
    "def sNp(images, ratio_S, ratio_N):\n",
    "    shp = K.shape(images)[1:]\n",
    "    mask_sel = K.random_bernoulli(shape=shp, p=ratio_S)\n",
    "    mask_noise = K.random_bernoulli(shape=shp, p=ratio_N) # higher p = more 1's\n",
    "    out = images * (1-mask_sel) + mask_noise * mask_sel\n",
    "    return out\n",
    "\n",
    "def train_step(images):\n",
    "    clean_output = next(iter(images))\n",
    "#     print('Max of image')\n",
    "#     print(np.amax(clean_output))\n",
    "    clean_observed_noise = clean_output + tf.random.normal(shape=tf.shape(clean_output))/15.0 # Right side\n",
    "#     print('Max of clean plus noise')\n",
    "#     print(np.amax(clean_observed_noise))\n",
    "    error_input = clean_observed_noise + sNp(clean_output, 0.1, 0.8)# TODO - use error images\n",
    "#     print('max of error')\n",
    "#     print(np.amax(error_input))\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        error_output = DnCNN(error_input)\n",
    "        \n",
    "        residual = clean_observed_noise - error_output\n",
    "#         tf.print(residual)\n",
    "#         print('Sum of Residual:')\n",
    "#         print(tf.reduce_sum(residual).numpy())\n",
    "        loss = (1.0/BATCH_SIZE)*tf.nn.l2_loss(residual)\n",
    "#         print('Loss')\n",
    "#         tf.print(loss)\n",
    "        eval_psnr = tf.image.psnr(error_output, clean_observed_noise, 1.0)\n",
    "    \n",
    "        grads = tape.gradient(loss, DnCNN.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, DnCNN.trainable_variables))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "#@tf.function\n",
    "def train(dataset, epochs):\n",
    "    Loss = float('inf')*np.ones(10)\n",
    "    best = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        OG_loss = 0.0\n",
    "        num_batch = 0\n",
    "        \n",
    "        for image_batch in dataset:\n",
    "            loss = train_step(image_batch)\n",
    "            #tf.print(loss)\n",
    "            OG_loss = OG_loss + loss\n",
    "            num_batch = num_batch + 1\n",
    "            print('I might be working...batch numba=',num_batch)\n",
    "            \n",
    "        # Produce images for the GIF as we go\n",
    "        OG_loss = OG_loss / num_batch\n",
    "        print('OG_loss:')\n",
    "        tf.print(OG_loss)\n",
    "        generate_and_save_images(DnCNN, epoch + 1, seed) # testing images\n",
    "\n",
    "        if (best > OG_loss):\n",
    "            best = OG_loss\n",
    "            best_epoch = epoch\n",
    "        print(epoch)\n",
    "        if epoch < 10:\n",
    "            print('wtf')\n",
    "            Loss[epoch] = OG_loss\n",
    "        else:\n",
    "            Loss = np.roll(Loss,-1)\n",
    "            Loss[-1] = OG_loss\n",
    "\n",
    "        if epoch > 10:\n",
    "            print('wtf - how')\n",
    "            if (np.mean(Loss[5:9]) > np.mean(Loss[0:4])):\n",
    "                print('Stopped at epoch=', epoch + 1, \" Best epoch was=\", best_epoch + 1);\n",
    "                break\n",
    "\n",
    "        # Save the model every 15 epochs\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "          checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "    \n",
    "    return(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run StridedSlice: Dst tensor is not initialized. [Op:StridedSlice] name: strided_slice/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-507a475a6725>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mseed1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_test_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mseed2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_test_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mseed3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_test_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mseed4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_test_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mseed5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_test_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-ee171de84176>\u001b[0m in \u001b[0;36mget_image\u001b[0;34m(image_batch)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0mvar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[1;32m   1194\u001b[0m       \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m       shrink_axis_mask=shrink_axis_mask)\n\u001b[0m\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m   \u001b[0mparent_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[1;32m  10318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10319\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10320\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10321\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10322\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6842\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6843\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6844\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run StridedSlice: Dst tensor is not initialized. [Op:StridedSlice] name: strided_slice/"
     ]
    }
   ],
   "source": [
    "seed1 = get_image(next(iter(error_test_dataset)))\n",
    "seed2 = get_image(next(iter(error_test_dataset)))\n",
    "seed3 = get_image(next(iter(error_test_dataset)))\n",
    "seed4 = get_image(next(iter(error_test_dataset)))\n",
    "seed5 = get_image(next(iter(error_test_dataset)))\n",
    "seed6 = get_image(next(iter(error_test_dataset)))\n",
    "seed7 = get_image(next(iter(error_test_dataset)))\n",
    "seed8 = get_image(next(iter(error_test_dataset)))\n",
    "seed9 = get_image(next(iter(error_test_dataset)))\n",
    "seed = tf.stack([seed1, seed2, seed3, seed4, seed5, seed6, seed7, seed8, seed9],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=200\n",
    "\n",
    "max_epoch = train(error_train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore last checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "  return PIL.Image.open('NAC_outputs/epoch_images/image_at_epoch_{:04d}.png'.format(epoch_no))\n",
    "\n",
    "print(max_epoch)\n",
    "display_image(max_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim_file = 'NAC_outputs/NAC_high_res.gif'\n",
    "import imageio\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "  filenames = glob.glob('NAC_outputs/epoch_images/image*.png')\n",
    "  filenames = sorted(filenames)\n",
    "  for filename in filenames:\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "  image = imageio.imread(filename)\n",
    "  writer.append_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_docs.vis.embed as embed\n",
    "embed.embed_file(anim_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(os.getcwd())\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "d = today.strftime(\"%b-%d-%Y\")\n",
    "# print(d)\n",
    "\n",
    "DnCNN.save('models/'+d+'_NAC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
